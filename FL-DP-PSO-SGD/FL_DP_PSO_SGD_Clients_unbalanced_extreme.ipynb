{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caua-sathler/FLPSO-SGD_algorithm/blob/main/FL-DP-PSO-SGD/FL_DP_PSO_SGD_Clients_unbalanced_extreme.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnYrR-lgoEP5",
        "outputId": "db54ffe2-802d-46b4-d1c1-0a54a416f1c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opacus in /usr/local/lib/python3.11/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.15 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.23.5)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->opacus) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opacus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YzFuU81wq65",
        "outputId": "718a8e64-a0be-405f-8441-bdecdecdc0e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install --upgrade scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA6XhnylxbJl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, Subset, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import random\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
        "from opacus.accountants.utils import get_noise_multiplier\n",
        "from opacus.accountants import RDPAccountant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BflMFLxxfuo"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, device, input_size=28*28, hidden_size=256, num_classes=10):\n",
        "        super(MLP, self).__init__()\n",
        "        self.device = device\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Achatar o tensor de entrada\n",
        "        y = self.fc1(x)\n",
        "        y = self.relu(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.fc3(y)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0bZVaFyxhzn",
        "outputId": "36cedc78-f4cd-4143-e956-5fa3acb4429b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on cuda\n"
          ]
        }
      ],
      "source": [
        "# Definições dos hiperparâmetros\n",
        "NUM_CLIENTES = 5\n",
        "NUM_PARTICULAS = 25\n",
        "NUM_RODADAS = 10\n",
        "NUM_DIGITOS = 10\n",
        "#INERCIA, C1, C2 = 0.8, 1.5,  1.9\n",
        "INERCIA, C1, C2 = 0.9, 0.8,  0.9\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 240\n",
        "print(f'training on {DEVICE}')\n",
        "\n",
        "# Criando o modelo global\n",
        "modelo_global = MLP(DEVICE)\n",
        "criterio = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF2COcYRxmin",
        "outputId": "4e2be581-6fef-44a9-81af-2754f1f89937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n",
            "Cliente 0: {0: 3923, 1: 4742, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500}\n",
            "Cliente 1: {0: 500, 1: 500, 2: 3958, 3: 4131, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500}\n",
            "Cliente 2: {0: 500, 1: 500, 2: 500, 3: 500, 4: 3842, 5: 3421, 6: 500, 7: 500, 8: 500, 9: 500}\n",
            "Cliente 3: {0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 3918, 7: 4265, 8: 500, 9: 500}\n",
            "Cliente 4: {0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 3851, 9: 3949}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rodada 1/10: Cliente 3 enviou os pesos!\n",
            "Erro Global Atualizado: 0.5205\n",
            "Epsilon 10 desbalanceado extremo \n",
            "Teste -> Perda:  1.1095, Acurácia:  0.6859%\n",
            "\n",
            "Rodada 2/10: Cliente 0 enviou os pesos!\n",
            "Erro Global Atualizado: 0.3016\n",
            "Epsilon 10 desbalanceado extremo \n",
            "Teste -> Perda:  0.6345, Acurácia:  0.8380%\n",
            "\n",
            "Rodada 3/10: Cliente 2 enviou os pesos!\n",
            "Erro Global Atualizado: 0.3973\n",
            "Epsilon 10 desbalanceado extremo \n",
            "Teste -> Perda:  0.6957, Acurácia:  0.8475%\n",
            "\n",
            "Rodada 4/10: Cliente 2 enviou os pesos!\n",
            "Erro Global Atualizado: 0.4132\n",
            "Epsilon 10 desbalanceado extremo \n",
            "Teste -> Perda:  0.7108, Acurácia:  0.8614%\n",
            "\n",
            "Rodada 5/10: Cliente 1 enviou os pesos!\n",
            "Erro Global Atualizado: 0.3966\n",
            "Epsilon 10 desbalanceado extremo \n",
            "Teste -> Perda:  0.7431, Acurácia:  0.8718%\n",
            "\n",
            "Rodada 6/10: Cliente 2 enviou os pesos!\n",
            "Erro Global Atualizado: 0.4159\n",
            "Epsilon 10 desbalanceado extremo \n",
            "Teste -> Perda:  0.8164, Acurácia:  0.8681%\n",
            "\n",
            "Rodada 7/10: Cliente 0 enviou os pesos!\n",
            "Erro Global Atualizado: 0.2776\n",
            "Epsilon 10 desbalanceado extremo \n",
            "Teste -> Perda:  0.5705, Acurácia:  0.8706%\n",
            "\n",
            "Rodada 8/10: Cliente 3 enviou os pesos!\n",
            "Erro Global Atualizado: 0.3097\n",
            "Epsilon 10 desbalanceado extremo \n",
            "Teste -> Perda:  0.6633, Acurácia:  0.8720%\n",
            "\n",
            "Rodada 9/10: Cliente 3 enviou os pesos!\n",
            "Erro Global Atualizado: 0.3382\n",
            "Epsilon 10 desbalanceado extremo \n",
            "Teste -> Perda:  0.7745, Acurácia:  0.8681%\n",
            "\n",
            "Rodada 10/10: Cliente 2 enviou os pesos!\n",
            "Erro Global Atualizado: 0.3718\n",
            "Epsilon 10 desbalanceado extremo \n",
            "Teste -> Perda:  0.6692, Acurácia:  0.8707%\n",
            "\n",
            "Acurácia média:  0.85\n",
            "Treinamento Federado Finalizado!\n",
            "9.990324643312247\n",
            "9.991894315717618\n",
            "9.99697960029635\n",
            "9.991894315717618\n",
            "9.99635871774549\n"
          ]
        }
      ],
      "source": [
        "numero = random.randint(70, 140)\n",
        "print(numero)\n",
        "random.seed(numero)\n",
        "torch.manual_seed(numero)\n",
        "torch.cuda.manual_seed(numero)\n",
        "\n",
        "class Particula:\n",
        "    def __init__(self, particle_id, modelo_cliente):\n",
        "        self.particle_id = particle_id\n",
        "        #self.pesos = copy.deepcopy(modelo_cliente.state_dict())\n",
        "        self.pesos = {f\"_module.{key}\": value.clone() for key, value in modelo_cliente.state_dict().items()}\n",
        "        self.device = modelo_cliente.device\n",
        "\n",
        "        # Adiciona ruído leve nos pesos para quebrar simetria inicial\n",
        "        for name in self.pesos:\n",
        "            self.pesos[name] += 0.01 * torch.randn_like(self.pesos[name])\n",
        "            #self.pesos[name] += 0.001 * torch.randn_like(self.pesos[name])\n",
        "\n",
        "        self.melhor_pesos = copy.deepcopy(self.pesos)\n",
        "        self.melhor_erro = float('inf')\n",
        "        self.velocidade = {name: torch.zeros_like(param) for name, param in self.pesos.items()}\n",
        "\n",
        "\n",
        "    def atualizar_pso(self, global_best_pesos, INERCIA, C1, C2):\n",
        "        MAX_VELOCITY = 0.1  # Limite para evitar oscilações grandes\n",
        "        if(round == 0):\n",
        "          global_best_weights_adjusteds = {f\"_module.{key}\": value for key, value in global_best_pesos.items()}\n",
        "        for name in self.pesos:\n",
        "            local_rand = random.random()\n",
        "            global_rand = random.random()\n",
        "            self.velocidade[name] = (\n",
        "                INERCIA * self.velocidade[name]\n",
        "                + C1 * local_rand * (self.melhor_pesos[name] - self.pesos[name])\n",
        "                + C2 * global_rand * (global_best_pesos[name] - self.pesos[name])\n",
        "            )\n",
        "\n",
        "            # Clipping da velocidade\n",
        "            self.velocidade[name] = torch.clamp(self.velocidade[name], -MAX_VELOCITY, MAX_VELOCITY)\n",
        "\n",
        "            # Atualiza os pesos com a nova velocidade\n",
        "            self.pesos[name] += self.velocidade[name]\n",
        "\n",
        "    def avaliar_perda(self, modelo_cliente, criterio, dados):\n",
        "        modelo_cliente.load_state_dict(self.pesos)\n",
        "        modelo_cliente.eval()\n",
        "        total_loss = 0\n",
        "        device = next(modelo_cliente.parameters()).device\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dados:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = modelo_cliente(inputs)\n",
        "                loss = criterio(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "\n",
        "        return total_loss / len(dados)\n",
        "\n",
        "\n",
        "\n",
        "class Cliente:\n",
        "    def __init__(self, cliente_id, modelo_global, dados, test, num_particulas):\n",
        "        self.cliente_id = cliente_id\n",
        "        self.modelo = copy.deepcopy(modelo_global)  # Cada cliente tem seu próprio modelo\n",
        "        self.dados = dados\n",
        "        self.test = test\n",
        "        self.num_particulas = num_particulas\n",
        "        self.particulas = []\n",
        "        self.melhor_particula = None\n",
        "        self.inicializar_particulas(num_particulas)\n",
        "        self.optimizer = optim.Adam(self.modelo.parameters(), lr=0.005, weight_decay=1e-5)\n",
        "\n",
        "    def inicializar_particulas(self, num_particulas):\n",
        "        \"\"\"Cria um conjunto de partículas associadas ao cliente.\"\"\"\n",
        "        self.particulas = [Particula(i, self.modelo) for i in range(num_particulas)]\n",
        "\n",
        "    def treinar_com_pso(self, INERCIA, C1, C2, global_best_pesos, criterio):\n",
        "        \"\"\"Treina as partículas usando PSO e atualiza a melhor partícula local.\"\"\"\n",
        "\n",
        "        for particula in self.particulas:\n",
        "            particula.atualizar_pso(global_best_pesos, INERCIA, C1, C2)\n",
        "            erro = particula.avaliar_perda(self.modelo, criterio, self.dados)\n",
        "            if erro < particula.melhor_erro:\n",
        "                particula.melhor_erro = erro\n",
        "                particula.melhor_pesos = copy.deepcopy(particula.pesos)\n",
        "\n",
        "        self.selecionar_melhor_particula()\n",
        "        # modelo_global.load_state_dict(self.melhor_particula.pesos)\n",
        "\n",
        "    def refinar_com_adam(self, criterio):\n",
        "        \"\"\"Refina os pesos da melhor partícula usando Adam.\"\"\"\n",
        "        self.modelo.load_state_dict(self.melhor_particula.melhor_pesos)\n",
        "        # self.modelo.load_state_dict(modelo_global.state_dict())\n",
        "        device = next(self.modelo.parameters()).device\n",
        "\n",
        "        self.modelo.train()\n",
        "        for i in range(1):\n",
        "          with BatchMemoryManager(data_loader=self.dados, max_physical_batch_size=BATCH_SIZE, optimizer=self.optimizer) as new_data_loader:\n",
        "            for inputs, labels in new_data_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.modelo(inputs)\n",
        "                loss = criterio(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "        self.melhor_particula.melhor_pesos = copy.deepcopy(self.modelo.state_dict())\n",
        "        self.melhor_particula.melhor_erro = self.calcular_loss(self.modelo, criterio, self.dados)\n",
        "\n",
        "    def calcular_loss(self, modelo, criterio, dados):\n",
        "        self.modelo.eval()\n",
        "        total_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dados:\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = modelo(inputs)\n",
        "                loss = criterio(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dados)\n",
        "\n",
        "    def selecionar_melhor_particula(self):\n",
        "        \"\"\"Seleciona a melhor partícula do cliente.\"\"\"\n",
        "        self.melhor_particula = min(self.particulas, key=lambda p: p.melhor_erro)\n",
        "\n",
        "\n",
        "def treinar_federado(modelo_global, clientes, criterio, num_rodadas, INERCIA, C1, C2, testloader):\n",
        "    \"\"\"Treina os clientes localmente e sincroniza com o servidor central, validando a acurácia.\"\"\"\n",
        "\n",
        "    melhor_peso_global = {f\"_module.{key}\": value.clone() for key, value in modelo_global.state_dict().items()}\n",
        "    melhor_erro_global = float('inf')\n",
        "    soma = 0.0;\n",
        "    for rodada in range(num_rodadas):\n",
        "        resultados_rodada = []\n",
        "\n",
        "        for cliente in clientes:\n",
        "          cliente.treinar_com_pso(INERCIA, C1, C2, melhor_peso_global, criterio)  # Treino com PSO\n",
        "          cliente.refinar_com_adam(criterio)  # Refinamento com Adam\n",
        "          erro_cliente = cliente.melhor_particula.melhor_erro  # Obtém o melhor erro do cliente\n",
        "          pesos_cliente = cliente.melhor_particula.melhor_erro  # Obtém os pesos do modelo do cliente\n",
        "          resultados_rodada.append((cliente.cliente_id, erro_cliente))\n",
        "\n",
        "        resultados_sorted = sorted(resultados_rodada, key=lambda x: x[1])\n",
        "        top_3_results = resultados_sorted[:3]\n",
        "\n",
        "        melhor_cliente = random.choice(top_3_results)\n",
        "        melhor_cliente_id = melhor_cliente[0]\n",
        "        melhor_erro_cliente = melhor_cliente[1]\n",
        "\n",
        "        melhor_peso_global = copy.deepcopy(clientes[melhor_cliente_id].melhor_particula.melhor_pesos)\n",
        "        melhor_peso_global_adjusted =  {key.replace(\"_module.\", \"\"): value for key, value in melhor_peso_global.items()}\n",
        "        melhor_erro_global = melhor_erro_cliente\n",
        "\n",
        "        modelo_global.load_state_dict(melhor_peso_global_adjusted)\n",
        "\n",
        "        test_loss, test_accuracy = avaliar_modelo(modelo_global, criterio, testloader)\n",
        "        soma += test_accuracy\n",
        "\n",
        "        # if (rodada+1) % 10 == 0:\n",
        "        print(f\"Rodada {rodada+1}/{num_rodadas}: Cliente {melhor_cliente_id} enviou os pesos!\")\n",
        "        print(f\"Erro Global Atualizado: {melhor_erro_global:.4f}\")\n",
        "        print(\"Epsilon 10 desbalanceado extremo \")\n",
        "        print(f\"Teste -> Perda: {test_loss: .4f}, Acurácia: {test_accuracy: .4f}%\\n\")\n",
        "\n",
        "    print(f\"Acurácia média: {soma/num_rodadas: .2f}\")\n",
        "    print(\"Treinamento Federado Finalizado!\")\n",
        "\n",
        "def avaliar_modelo(modelo, criterio, testloader):\n",
        "    \"\"\"Avalia o modelo global no conjunto de teste.\"\"\"\n",
        "    modelo.eval()  # Modo de avaliação\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = modelo(inputs)\n",
        "            loss = criterio(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    test_loss = total_loss / len(testloader)\n",
        "    test_accuracy = (correct / total_samples)\n",
        "\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "mnist_test = torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
        "X_test = mnist_test.data.view(-1, 28*28).numpy()  # Flatten\n",
        "y_test = mnist_test.targets.numpy()\n",
        "\n",
        "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
        "X_train = mnist_train.data.view(-1, 28*28).numpy()  # Flatten\n",
        "y_train = mnist_train.targets.numpy()\n",
        "\n",
        "# Normalização (como foi feito com o Iris)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Converter para tensores\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Criar datasets\n",
        "trainset = TensorDataset(X_train, y_train)\n",
        "testset = TensorDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "client_digit_mapping = {\n",
        "    0: [0, 1],\n",
        "    1: [2, 3],\n",
        "    2: [4, 5],\n",
        "    3: [6, 7],\n",
        "    4: [8, 9]\n",
        "}\n",
        "\n",
        "def create_unbalanced_subsets(dataset, num_clients, min_per_digit=500):\n",
        "    targets = dataset.tensors[1].numpy()\n",
        "\n",
        "    # Índices das classes disponíveis\n",
        "    class_indices = {digit: np.where(targets == digit)[0] for digit in range(10)}\n",
        "    for digit in class_indices:\n",
        "        np.random.shuffle(class_indices[digit])\n",
        "\n",
        "    # Índices dos clientes\n",
        "    client_indices = {i: [] for i in range(num_clients)}\n",
        "\n",
        "    # Distribui inicialmente \"min_per_digit\" amostras por dígito para cada cliente\n",
        "    for digit in range(10):\n",
        "        available_indices = class_indices[digit]\n",
        "\n",
        "        min_total_samples = num_clients * min_per_digit\n",
        "        if len(available_indices) < min_total_samples:\n",
        "            raise ValueError(f\"Número insuficiente de amostras para o dígito {digit}\")\n",
        "\n",
        "        for client_id in range(num_clients):\n",
        "            start_idx = client_id * min_per_digit\n",
        "            end_idx = start_idx + min_per_digit\n",
        "            selected = available_indices[start_idx:end_idx]\n",
        "            client_indices[client_id].extend(selected.tolist())\n",
        "\n",
        "        class_indices[digit] = available_indices[min_total_samples:]\n",
        "\n",
        "    # Distribuição exclusiva do restante dos dígitos por cliente\n",
        "    digits_per_client = [(i*2, i*2+1) for i in range(num_clients)]\n",
        "\n",
        "    for client_id, (digit_a, digit_b) in enumerate(digits_per_client):\n",
        "        for digit in [digit_a, digit_b]:\n",
        "            remaining_indices = class_indices[digit]\n",
        "            client_indices[client_id].extend(remaining_indices.tolist())\n",
        "            class_indices[digit] = []  # Zera para garantir que não sejam reutilizados\n",
        "\n",
        "    # Criar subsets\n",
        "    subsets = [Subset(dataset, client_indices[i]) for i in range(num_clients)]\n",
        "\n",
        "    # Mostrar distribuição final\n",
        "    for client_id in range(num_clients):\n",
        "        client_targets = targets[client_indices[client_id]]\n",
        "        digit_counts = {digit: np.sum(client_targets == digit) for digit in range(10)}\n",
        "        print(f\"Cliente {client_id}: {digit_counts}\")\n",
        "\n",
        "    return subsets\n",
        "\n",
        "# Criar DataLoaders para cada subset (cliente)\n",
        "train_subsets = create_unbalanced_subsets(trainset, NUM_CLIENTES)\n",
        "\n",
        "# Criar DataLoaders para cada subset (cliente)\n",
        "trainloaders = [DataLoader(train_subsets[i], batch_size=240, shuffle=True) for i in range(NUM_CLIENTES)]\n",
        "\n",
        "testloader = DataLoader(testset, batch_size=240, shuffle=False)\n",
        "\n",
        "clientes = [Cliente(i, modelo_global, trainloaders[i], testloader, NUM_PARTICULAS) for i in range(NUM_CLIENTES)]\n",
        "\n",
        "privacy_engines = [PrivacyEngine() for i in range(NUM_CLIENTES)]\n",
        "\n",
        "for i in range (NUM_CLIENTES):\n",
        "    clientes[i].modelo, clientes[i].optimizer, clientes[i].dados = privacy_engines[i].make_private_with_epsilon(\n",
        "    module=clientes[i].modelo,\n",
        "    optimizer=clientes[i].optimizer,\n",
        "    data_loader=clientes[i].dados,\n",
        "    epochs = NUM_RODADAS,\n",
        "    target_epsilon = 10,\n",
        "    target_delta = 1e-5,\n",
        "    max_grad_norm=1.0\n",
        ")\n",
        "\n",
        "#Executando o treinamento federado\n",
        "treinar_federado(modelo_global, clientes, criterio, NUM_RODADAS, INERCIA, C1, C2, testloader)\n",
        "for i in range(NUM_CLIENTES):\n",
        "  print(privacy_engines[i].get_epsilon(delta = 1e-5))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}